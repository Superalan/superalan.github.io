<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kubernetes 1.5版本搭建]]></title>
    <url>%2F2017%2F01%2F05%2Fkubernetes-1.5-setup%2F</url>
    <content type="text"><![CDATA[kubernetes 1.5搭建##升级内核 使用如下脚本安装更新系统内核。 1234567891011121314151617181920212223#!/bin/bashset -eecho &quot;Upgrade Kernel Started&quot;mkdir -p /home/work/dockermkdir -p /home/work/tmpcd /home/work/tmpuname -r#下载公司3.10kernelwget ftp://nmg02-dev-sunyingnan.nmg02.baidu.com:/home/work/opdir/kernel/kernel-3.10.0_1-0-0-8.tgztar zxf kernel-3.10.0_1-0-0-8.tgzcd /home/work/tmp/kernel-3.10.0_1-0-0-8set +e#这一步需要等待较长时间，可以忽略输出的错误信息./auto_changekernel.shset -e#挂载cgroup fscd /home/work/tmpwget --no-check-certificate https://raw.githubusercontent.com/tianon/cgroupfs-mount/master/cgroupfs-mountchmod +x cgroupfs-mount./cgroupfs-mountecho &quot;Upgrade Kernel Finished&quot;#重新启动主机，整个重启过程约为10分钟，遇到问题诸如ssh: connect to host xxx.xxx.xxx.xxx port 22: Connection timed out，请耐心等待shutdown -r now ##安装docker 12345678910111213141516171819202122232425262728293031323334353637383940#!/bin/bashset -e#创建docker文件系统目录echo &quot;Installation Started&quot;mkdir -p /home/work/dockercd /home/work#下载docker1.10.0二进制文件if [[ ! -f &quot;/usr/bin/docker&quot; ]];then wget http://10.107.26.32:8000/docker-1.10.0-rc1 -O /usr/bin/docker chmod +x /usr/bin/dockerfi#设置docker网络，创建网桥，使用10.0.3.1/24私有网段，和matrix保持一致if [[ $(brctl show | grep &apos;docker0&apos; | wc -l) -lt 1 ]];then brctl addbr docker0 if [[ $(ip addr show | grep &apos;10.0.3.1&apos; | wc -l) -lt 1 ]];then ip addr add 10.0.3.1/24 dev docker0 fi ip link set dev docker0 upfi#挂载cgroup fs cd /home/work/tmpwget --no-check-certificate https://raw.githubusercontent.com/tianon/cgroupfs-mount/master/cgroupfs-mountchmod +x cgroupfs-mount./cgroupfs-mount#启动docker daemonif [[ $(ps aux | grep &apos;docker&apos; | wc -l) -lt 2 ]];then nohup /usr/bin/docker daemon -b docker0 -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --insecure-registry registry.baidu.com -g /home/work/docker &amp; sleep 2 while [[ $(ps aux | grep &apos;docker&apos; | wc -l) -lt 2 ]];do sleep 2 donefi#下载docker compose 1.5.2二进制文件if [[ ! -f &quot;/usr/bin/docker-compose&quot; ]];then wget http://10.107.26.32:8000/docker-compose-1.5.2 -O /usr/bin/docker-compose chmod +x /usr/bin/docker-composefi#下载baidu官方centos6u3基础镜像，所有docker镜像依赖于此镜像docker pull registry.baidu.com/public/centos6u3:1.0.1echo &quot;Installation Finished&quot; ##获取k8s组件 从官方release获取 123# 当前release版本中不包含编译后的二进制程序，只包含源码$ wget https://github.com/kubernetes/kubernetes/releases/download/v1.5.3/kubernetes.tar.gz &amp;&amp; tar -zxf kubernetes.tar.gz# 解压后，可以使用官方脚本下载获取官方编译的二进制：kubernetes/cluster/get-kube-binaries.sh 从开发机获取 我将之前将下载的程序进行了整理，并加入了启停脚本，因此可以更为方便的构建。 12# 获取组件$ wget ftp://nmg02-dev-sunyingnan.nmg02.baidu.com:/home/work/opdir/kubernetes/kubernetes-1.5.3-binary/kubernetes-1.5.3-binary.tar.gz 解压后启动k8s所有组件，组件中包含了etcd，可以先行启动etcd，以便后续组件正常启动。其中kubelet中cni的部分先去除，否则可能会导致组件启动异常。 ##搭建calico 下载calico命令行，并且执行node节点启动。 123456# Download and install `calicoctl`wget https://github.com/projectcalico/calicoctl/releases/download/v1.1.0/calicoctlsudo chmod +x calicoctl# Run the calico/node containersudo ETCD_ENDPOINTS=http://&lt;ETCD_IP&gt;:&lt;ETCD_PORT&gt; ./calicoctl node run 安装calico CNI插件： 123wget -N -P /opt/cni/bin https://github.com/projectcalico/cni-plugin/releases/download/v1.6.1/calicowget -N -P /opt/cni/bin https://github.com/projectcalico/cni-plugin/releases/download/v1.6.1/calico-ipamchmod +x /opt/cni/bin/calico /opt/cni/bin/calico-ipam 配置cni配置文件： 123456789101112131415161718mkdir -p /etc/cni/net.dcat &gt;/etc/cni/net.d/10-calico.conf &lt;&lt;EOF&#123; &quot;name&quot;: &quot;calico-k8s-network&quot;, &quot;type&quot;: &quot;calico&quot;, &quot;etcd_endpoints&quot;: &quot;http://&lt;ETCD_IP&gt;:&lt;ETCD_PORT&gt;&quot;, &quot;log_level&quot;: &quot;info&quot;, &quot;ipam&quot;: &#123; &quot;type&quot;: &quot;calico-ipam&quot; &#125;, &quot;policy&quot;: &#123; &quot;type&quot;: &quot;k8s&quot; &#125;, &quot;kubernetes&quot;: &#123; &quot;kubeconfig&quot;: &quot;&lt;/PATH/TO/KUBECONFIG&gt;&quot; &#125;&#125;EOF 除了cni插件本身之外，k8s还需要loopback插件。 123wget https://github.com/containernetworking/cni/releases/download/v0.3.0/cni-v0.3.0.tgztar -zxvf cni-v0.3.0.tgzsudo cp loopback /opt/cni/bin/ 安装calico network policy controller, controller相关配置文件如下(修改ETCD_ENDPOINTS)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# Create this manifest using kubectl to deploy# the Calico policy controller on Kubernetes.# It deploys a single instance of the policy controller.apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: calico-policy-controller namespace: kube-system labels: k8s-app: calico-policyspec: # Only a single instance of the policy controller should be # active at a time. Since this pod is run as a Deployment, # Kubernetes will ensure the pod is recreated in case of failure, # removing the need for passive backups. replicas: 1 strategy: type: Recreate template: metadata: name: calico-policy-controller namespace: kube-system labels: k8s-app: calico-policy spec: hostNetwork: true containers: - name: calico-policy-controller # Make sure to pin this to your desired version. image: quay.io/calico/kube-policy-controller:&#123;&#123;site.data.versions[page.version].first.components[&quot;calico/kube-policy-controller&quot;].version&#125;&#125; env: # Configure the policy controller with the location of # your etcd cluster. - name: ETCD_ENDPOINTS value: &quot;&lt;ETCD_ENDPOINTS&gt;&quot; # Location of the Kubernetes API - this shouldn&apos;t need to be # changed so long as it is used in conjunction with # CONFIGURE_ETC_HOSTS=&quot;true&quot;. - name: K8S_API value: &quot;https://kubernetes.default:443&quot; # Configure /etc/hosts within the container to resolve # the kubernetes.default Service to the correct clusterIP # using the environment provided by the kubelet. # This removes the need for KubeDNS to resolve the Service. - name: CONFIGURE_ETC_HOSTS value: &quot;true&quot; 使用kubectl执行创建，创建完成后可以看到对应的pod： 1234$ kubectl create -f policy-controller.yaml$ kubectl get pods --namespace=kube-systemNAME READY STATUS RESTARTS AGEcalico-policy-controller 1/1 Running 0 1m ##配置k8s组件 配置kubelet，确保在启动参数中添加如下参数： 123--network-plugin=cni--network-plugin-dir=/etc/cni/net.d--cluster-dns=11.1.1.200 其中dns的配置ip是我们实现约定的，这里可以先行配置，不影响组件启动。具体配置见下一部分。 配置kube-proxy，确保组件启动时选择代理模式是iptables： 12#启动时增加如下参数--proxy-mode=iptables ##配置dns k8s提供了dns的addon组件，可以在平台自身上托管相关组件进程。使用如下deployment配置在k8s平台上创建kube-dns组件，修改启动的配置kube-master-url到实际的api server地址。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: &quot;true&quot;spec: # replicas: not specified here: # 1. In order to make Addon Manager do not reconcile this replicas parameter. # 2. Default is 1. # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: rollingUpdate: maxSurge: 10% maxUnavailable: 0 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns annotations: scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos; scheduler.alpha.kubernetes.io/tolerations: &apos;[&#123;&quot;key&quot;:&quot;CriticalAddonsOnly&quot;, &quot;operator&quot;:&quot;Exists&quot;&#125;]&apos; spec: containers: - name: kubedns image: gcr.io/google_containers/kubedns-amd64:1.9 resources: # TODO: Set memory limits when we&apos;ve profiled the container for large # clusters, then set request = limit to keep this container in # guaranteed class. Currently, this container falls into the # &quot;burstable&quot; category so the kubelet doesn&apos;t backoff from restarting it. limits: memory: 170Mi requests: cpu: 100m memory: 70Mi livenessProbe: httpGet: path: /healthz-kubedns port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /readiness port: 8081 scheme: HTTP # we poll on pod startup for the Kubernetes master service and # only setup the /readiness HTTP server once that&apos;s available. initialDelaySeconds: 3 timeoutSeconds: 5 args: - --domain=cluster.local. - --dns-port=10053 - --kube-master-url=http://10.107.50.41:8080 # This should be set to v=2 only after the new image (cut from 1.5) has # been released, otherwise we will flood the logs. - --v=5 env: - name: PROMETHEUS_PORT value: &quot;10055&quot; ports: - containerPort: 10053 name: dns-local protocol: UDP - containerPort: 10053 name: dns-tcp-local protocol: TCP - containerPort: 10055 name: metrics protocol: TCP - name: dnsmasq image: gcr.io/google_containers/kube-dnsmasq-amd64:1.4 livenessProbe: httpGet: path: /healthz-dnsmasq port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - --cache-size=1000 - --no-resolv - --server=127.0.0.1#10053 ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP # see: https://github.com/kubernetes/kubernetes/issues/29055 for details resources: requests: cpu: 150m memory: 10Mi - name: dnsmasq-metrics image: gcr.io/google_containers/dnsmasq-metrics-amd64:1.0 livenessProbe: httpGet: path: /metrics port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - --v=2 - --logtostderr ports: - containerPort: 10054 name: metrics protocol: TCP resources: requests: memory: 10Mi - name: healthz image: gcr.io/google_containers/exechealthz-amd64:1.2 resources: limits: memory: 50Mi requests: cpu: 10m # Note that this container shouldn&apos;t really need 50Mi of memory. The # limits are set higher than expected pending investigation on #29688. # The extra memory was stolen from the kubedns container to keep the # net memory requested by the pod constant. memory: 50Mi args: - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 &gt;/dev/null - --url=/healthz-dnsmasq - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 &gt;/dev/null - --url=/healthz-kubedns - --port=8080 - --quiet ports: - containerPort: 8080 protocol: TCP dnsPolicy: Default # Don&apos;t use cluster DNS 创建好服务之后，再行创建service，这里需要确保使用的service ip与上面kubelet启动参数中的ip一致。 1234567891011121314151617181920apiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: &quot;true&quot; kubernetes.io/name: &quot;KubeDNS&quot;spec: selector: k8s-app: kube-dns clusterIP: 11.1.1.200 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP ##运行排查 ###启动kube-proxy报错 在当前版本中，如果service启用了service affinity且类型为ClientIp，则会用到iptables的扩展功能–reap（当前公司内核版本不支持）。启动k8s的时候徐彤默认创建了kubernetes这个service，是用到这个功能的，可以通过修改service属性跳过这个问题。 具体方法是执行kubelet edit service kubernetes,将Session Affinity设置为none。再重启proxy即可。 ###容器网络不通 calico网络的问题相对较好排查，查看route表基本就能查到问题。这里的问题是指除了路由规则之外，calico还具有policy的功能，可以用iptables做很多封禁策略。 当服务创建了一个service之后，calico会监测到这个service的创建，并且在policy中增加相应的规则。在搭建过程中，由于配置network policy controller错误，导致系统无法创建这个规则，因此在iptables层面封禁了容器网络的ip通信。这个问题需要格外关注。]]></content>
      <tags>
        <tag>kubernetes, 技术</tag>
      </tags>
  </entry>
</search>